{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor for PHP","text":"<p>Structured data extraction in PHP, powered by LLMs. Designed for simplicity, transparency, and control.</p>"},{"location":"#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to extract structured, validated data from unstructured text or OpenAI style chat sequence arrays. It is powered by Large Language Models.</p> <p>Instructor for PHP is inspired by the Instructor library for Python created by Jason Liu.</p> <p></p>"},{"location":"#instructor-in-other-languages","title":"Instructor in Other Languages","text":"<p>Check out implementations in other languages below:</p> <ul> <li>Python (original)</li> <li>Javascript (port)</li> <li>Elixir (port)</li> </ul> <p>If you want to port Instructor to another language, please reach out to us on Twitter we'd love to help you get started!</p>"},{"location":"#how-instructor-enhances-your-workflow","title":"How Instructor Enhances Your Workflow","text":"<p>Instructor introduces three key enhancements compared to direct API usage.</p>"},{"location":"#response-model","title":"Response Model","text":"<p>You just specify a PHP class to extract data into via the 'magic' of LLM chat completion. And that's it.</p> <p>Instructor reduces brittleness of the code extracting the information from textual data by leveraging structured LLM responses.</p> <p>Instructor helps you write simpler, easier to understand code - you no longer have to define lengthy function call definitions or write code for assigning returned JSON into target data objects.</p>"},{"location":"#validation","title":"Validation","text":"<p>Response model generated by LLM can be automatically validated, following set of rules. Currently, Instructor supports only Symfony validation.</p> <p>You can also provide a context object to use enhanced validator capabilities.</p>"},{"location":"#max-retries","title":"Max Retries","text":"<p>You can set the number of retry attempts for requests.</p> <p>Instructor will repeat requests in case of validation or deserialization error up to the specified number of times, trying to get a valid response from LLM.</p>"},{"location":"#explore","title":"Explore","text":"<ul> <li>Philosophy of Instructor</li> <li>Cookbook</li> <li>Internals of Instructor</li> </ul> <p>Also, check examples in the <code>examples</code> directory of this repository for fully working, tested code examples that you can execute from the command line and see Instructor in action.</p>"},{"location":"#additional-notes","title":"Additional Notes","text":"<p>PHP ecosystem does not (yet) have a strong equivalent of Pydantic, which is at the core of Instructor for Python.</p> <p>To provide an essential functionality we needed here Instructor for PHP leverages:</p> <ul> <li>base capabilities of PHP type system,</li> <li>PHP reflection,</li> <li>PHP DocBlock type hinting conventions,</li> <li>Symfony serialization and validation capabilities</li> </ul> <p>Currently, Instructor for PHP works with OpenAI API, but support for other models capable of function calling may be added in the future.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p>Instructor for PHP is compatible with PHP 8.2 or later and, due to minimal dependencies, should work with any framework of your choice.</p> <ul> <li>SaloonPHP - for handling communication with LLM API providers</li> <li>Symfony components - for validation, serialization and other utilities</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>If you want to help, check out some of the issues. They could be anything from code improvements, a guest blog post, or a new cookbook.</p>"},{"location":"contributing/#todos","title":"TODOs","text":"<ul> <li> Async support</li> <li> Open source LLM support</li> <li> Documentation - custom serializers, validators and LLMs</li> <li> Documentation - public vs protected / private fields</li> </ul>"},{"location":"data_model/","title":"Specifying Data Model","text":""},{"location":"data_model/#type-hints","title":"Type Hints","text":"<p>Use PHP type hints to specify the type of extracted data.</p> <p>Use nullable types to indicate that given field is optional.</p> <pre><code>&lt;?php\n\nclass Person {\n    public string $name;\n    public ?int $age;\n    public Address $address;\n}\n</code></pre> <p>Instructor will only fill in the fields that are public. Private and protected fields are ignored and their values are not going to be extracted (they will be left empty, with default values set as defined in your class).</p>"},{"location":"data_model/#docblock-type-hints","title":"DocBlock type hints","text":"<p>You can also use PHP DocBlock style comments to specify the type of extracted data. This is useful when you want to specify property types for LLM, but can't or don't want to enforce type at the code level.</p> <pre><code>&lt;?php\n\nclass Person {\n    /** @var string */\n    public $name;\n    /** @var int */\n    public $age;\n    /** @var Address $address person's address */\n    public $address;\n}\n</code></pre> <p>See PHPDoc documentation for more details on DocBlock: https://docs.phpdoc.org/3.0/guide/getting-started/what-is-a-docblock.html#what-is-a-docblock</p>"},{"location":"data_model/#using-docblocks-as-additional-instructions-for-llm","title":"Using DocBlocks as Additional Instructions for LLM","text":"<p>You can use PHP DocBlocks (/** */) to provide additional instructions for LLM at class or field level, for example to clarify what you expect or how LLM should process your data.</p> <p>Instructor extracts PHP DocBlocks comments from class and property defined and includes them in specification of response model sent to LLM.</p> <p>Using PHP DocBlocks instructions is not required, but sometimes you may want to clarify your intentions to improve LLM's inference results.</p> <pre><code>    /**\n     * Represents a skill of a person and context in which it was mentioned. \n     */\n    class Skill {\n        public string $name;\n        /** @var SkillType $type type of the skill, derived from the description and context */\n        public SkillType $type;\n        /** Directly quoted, full sentence mentioning person's skill */\n        public string $context;\n    }\n</code></pre>"},{"location":"data_model/#typed-collections-arrays","title":"Typed Collections / Arrays","text":"<p>PHP currently does not support generics or typehints to specify array element types.</p> <p>Use PHP DocBlock style comments to specify the type of array elements.</p> <pre><code>&lt;?php\n\nclass Person {\n    // ...\n}\n\nclass Event {\n    // ...\n    /** @var Person[] list of extracted event participants */\n    public array $participants;\n    // ...\n}\n</code></pre>"},{"location":"data_model/#complex-data-extraction","title":"Complex data extraction","text":"<p>Instructor can retrieve complex data structures from text. Your response model can contain nested objects, arrays, and enums.</p> <pre><code>&lt;?php\n\nuse Cognesy/Instructor;\n\n// define a data structures to extract data into\nclass Person {\n    public string $name;\n    public int $age;\n    public string $profession;\n    /** @var Skill[] */\n    public array $skills;\n}\n\nclass Skill {\n    public string $name;\n    public SkillType $type;\n}\n\nenum SkillType {\n    case Technical = 'technical';\n    case Other = 'other';\n}\n\n$text = \"Alex is 25 years old software engineer, who knows PHP, Python and can play the guitar.\";\n\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n); // client is passed explicitly, can specify e.g. different base URL\n\n// data is extracted into an object of given class\nassert($person instanceof Person); // true\n\n// you can access object's extracted property values\necho $person-&gt;name; // Alex\necho $person-&gt;age; // 25\necho $person-&gt;profession; // software engineer\necho $person-&gt;skills[0]-&gt;name; // PHP\necho $person-&gt;skills[0]-&gt;type; // SkillType::Technical\n// ...\n\nvar_dump($person);\n// Person {\n//     name: \"Alex\",\n//     age: 25,\n//     profession: \"software engineer\",\n//     skills: [\n//         Skill {\n//              name: \"PHP\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"Python\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"guitar\",\n//              type: SkillType::Other\n//         },\n//     ]\n// }\n</code></pre>"},{"location":"data_model/#scalar-values","title":"Scalar Values","text":"<p>Instructor can extract scalar values from text and assign them to your response model's properties.</p>"},{"location":"data_model/#example-string-result","title":"Example: String result","text":"<pre><code>&lt;?php\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::string(name: 'firstName'),\n);\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe(\"Jason\");\n</code></pre>"},{"location":"data_model/#example-integer-result","title":"Example: Integer result","text":"<pre><code>&lt;?php\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::integer('age'),\n);\n// expect($value)-&gt;toBeInt();\n// expect($value)-&gt;toBe(28);\n</code></pre>"},{"location":"data_model/#example-boolean-result","title":"Example: Boolean result","text":"<pre><code>&lt;?php\n\n$age = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::boolean(name: 'isAdult'),\n);\n// expect($age)-&gt;toBeBool();\n// expect($age)-&gt;toBe(true);\n</code></pre>"},{"location":"data_model/#example-float-result","title":"Example: Float result","text":"<pre><code>&lt;?php\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old and his 100m sprint record is 11.6 seconds.\",\n    responseModel: Scalar::float(name: 'recordTime'),\n);\n// expect($value)-&gt;toBeFloat();\n// expect($value)-&gt;toBe(11.6);\n</code></pre>"},{"location":"data_model/#example-select-one-of-the-options","title":"Example: Select one of the options","text":"<pre><code>&lt;?php\n\n$age = (new Instructor)-&gt;respond(\n    messages: \"His name is Dietmar, he is 28 years old and he lives in Germany.\",\n    responseModel: Scalar::select(\n        options: ['US citizen', 'Canada citizen', 'other'],\n        name: 'citizenshipGroup'\n    ),\n);\n// expect($age)-&gt;toBeString();\n// expect($age)-&gt;toBe('other');\n</code></pre>"},{"location":"data_model/#private-vs-public-object-field","title":"Private vs public object field","text":"<p>Instructor only sets public fields of the object with the data provided by LLM. Private and protected fields are left unchanged. If you want to access them directly after extraction, consider providing default values for them.</p> <p>See <code>examples/PrivateVsPublicFields/run.php</code> to check the details on the behavior of extraction for classes with private and public fields.</p>"},{"location":"help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p>"},{"location":"help/#material-discord-discord","title":":material-discord: Discord","text":"<p>The Discord is a great place to ask questions and get help from the community.</p>"},{"location":"help/#concepts","title":"Concepts","text":"<p>The concepts section explains the core concepts of Instructor and how to prompt with models.</p>"},{"location":"help/#cookbooks","title":"Cookbooks","text":"<p>The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.</p>"},{"location":"help/#blog","title":"Blog","text":"<p>The blog contains articles that explain how to use Instructor in different scenarios.</p>"},{"location":"help/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub discussions are useful for asking questions, your question and the answer will help everyone.</p>"},{"location":"help/#github-issues","title":"GitHub Issues","text":"<p>GitHub issues are useful for reporting bugs or requesting new features.</p>"},{"location":"help/#twitter","title":"Twitter","text":"<p>You can also reach out to me @ddebowczyk or Jason Liu if you have any questions or ideas.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>Installing Instructor is simple. Run following command in your terminal and you're on your way to a smoother data handling experience!</p> <pre><code>composer install cognesy/instructor-php\n</code></pre>"},{"location":"llm_providers/","title":"Supported LLM Providers","text":"<p>Only tested providers with examples are listed here.</p>"},{"location":"llm_providers/#openai","title":"OpenAI","text":"<p>OpenAI is the default provider that is called by Instructor unless user configures different one.</p> <p>Supported extraction modes:  - Mode::Tools (recommended)  - Mode::Json  - Mode::MdJson</p> <p>Majority of examples use OpenAI provider.</p>"},{"location":"llm_providers/#azure-openai","title":"Azure OpenAI","text":"<p>Azure is an alternative provider of OpenAI models. You can consider using it as a backup provider in case OpenAI is not available.</p> <p>Supported extraction modes: - Mode::Tools (recommended) - Mode::Json - Mode::MdJson</p> <p>Example: - <code>./examples/LLMSupportAzureOAI/run.php</code></p>"},{"location":"llm_providers/#ollama","title":"Ollama","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> </ul> <p>Example:  - <code>./examples/LLMSupportOllama/run.php</code></p>"},{"location":"llm_providers/#mistral-api","title":"Mistral API","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example:  - <code>./examples/LLMSupportMistral/run.php</code></p>"},{"location":"llm_providers/#anthropic","title":"Anthropic","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> </ul> <p>Mode::Tools is not supported yet.</p> <p>Example:  - <code>./examples/LLMSupportAnthropic/run.php</code></p>"},{"location":"llm_providers/#openrouter","title":"OpenRouter","text":"<p>You have to use our client adapter to work around the problem in the response format returned by OpenRouter for non-streamed requests.</p> <p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example:  - <code>./examples/LLMSupportOpenRouter/run.php</code></p>"},{"location":"model_options/","title":"LLM model and options","text":""},{"location":"model_options/#changing-llm-model-and-options","title":"Changing LLM model and options","text":"<p>You can specify model and other options that will be passed to OpenAI / LLM endpoint.</p> <pre><code>&lt;?php\n\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    model: 'gpt-3.5-turbo',\n    options: [\n        // custom temperature setting\n        'temperature' =&gt; 0.0\n        // ... other options\n    ],\n);\n</code></pre>"},{"location":"model_options/#providing-custom-client","title":"Providing custom client","text":"<p>You can pass a custom configured instance of client to the Instructor. This allows you to specify your own API key, base URI or organization.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Utils\\Env;\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenAIClient(\n    apiKey: $yourApiKey,\n    baseUri: 'https://api.openai.com/v1',\n    organization: '',\n    connectTimeout: 3,\n    requestTimeout: 30,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$person = $instructor-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    model: 'gpt-3.5-turbo',\n    options: ['temperature' =&gt; 0.0],\n);\n</code></pre>"},{"location":"modes/","title":"Extraction modes","text":"<p>Instructor supports several ways to extract data from the response. The default mode is <code>Mode::Tools</code>, which leverages OpenAI tool calls.</p> <p>Mode can be set via parameter of <code>Instructor::response()</code> or <code>Instructor::request()</code> methods.</p> <p><pre><code>use Cognesy\\Instructor\\Instructor;\n\n$instructor = new Instructor();\n\n$response = $instructor-&gt;respond(\n    messages: \"...\",\n    responseModel: ...,\n    ...,\n    mode: Mode::Json\n);\n</code></pre> Mode can be also set in <code>Request</code> object, if you are building it manually for  Instructor.</p> <pre><code>$request = new Request(\n    messages: \"...\",\n    responseModel: ...,\n    ...,\n    mode: Mode::Json\n);\n\n$response = $instructor-&gt;withRequest($request)-&gt;get();\n</code></pre>"},{"location":"modes/#modes","title":"Modes","text":""},{"location":"modes/#modetools","title":"<code>Mode::Tools</code>","text":"<p>This mode is the default one. It uses OpenAI tools to extract data from the response. It is the most reliable mode, but currently only available for OpenAI and Azure OpenAI LLMs.</p>"},{"location":"modes/#modeparalleltools","title":"<code>Mode::ParallelTools</code>","text":"<p>Not yet implemented. It will use OpenAI parallel tool calling to return multiple relevant response models in a single call.</p>"},{"location":"modes/#modejson","title":"<code>Mode::Json</code>","text":"<p>This mode uses OpenAI JSON mode. See <code>response_mode</code> in (OpenAI API Reference)[https://platform.openai.com/docs/api-reference/chat/create]</p>"},{"location":"modes/#modemdjson","title":"<code>Mode::MdJson</code>","text":"<p>In this mode Instructor asks LLM to answer with JSON object following provided schema and return answer as Markdown codeblock.</p> <p>It may improve the results for LLMs that have not been finetuned to respond with JSON as they are likely to be already trained on large amounts of programming docs and have seen a lot of properly formatted JSON objects within MD codeblocks.</p>"},{"location":"modes/#modefunctions","title":"<code>Mode::Functions</code>","text":"<p>Not likely to be implemented. OpenAI deprecated this mode. To be researched if it is used by any OS models (which would justify the effort).</p>"},{"location":"modes/#modeyaml","title":"<code>Mode::Yaml</code>","text":"<p>Not implemented. To be researched as a potential alternative to MdJson mode. Robustness vs MdJson mode is to be evaluated, but it may be easier for smaller LLMs to return correct data in this format due to simpler syntax.</p>"},{"location":"partials/","title":"Partial results","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated.</p> <p>You can use it to improve user experience by updating the UI with partial data before the full response is received.</p> <p>This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive partial results define <code>onPartialUpdate()</code> callback that will be called on every update of the deserializad object.</p> <p>Instructor is smart about updates, it calculates and compares hashes of the previous and newly  deserialized version of the model, so you won't get them on every token received, but only when any property of the object is updated.</p> <pre><code>use Cognesy\\Instructor;\n\nfunction updateUI($person) {\n    // Here you get partially completed Person object update UI with the partial result\n}\n\n$person = (new Instructor)-&gt;request(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n    options: ['stream' =&gt; true]\n)-&gt;onPartialUpdate(\n    fn($partial) =&gt; updateUI($partial)\n)-&gt;get();\n\n// Here you get completed and validated Person object\n$this-&gt;db-&gt;save($person); // ...for example: save to DB\n</code></pre> <p>Partially updated data is not validated while they are received and deserialized.</p> <p>The object returned from <code>get()</code> call is fully validated, so you can safely work with it, e.g. save it to the database.</p>"},{"location":"scalars/","title":"Extracting Scalar Values","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p> <pre><code>use Cognesy\\Instructor;\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::integer('age'),\n);\n\nvar_dump($value);\n// int(28)\n</code></pre> <p>In this example, we're extracting a single integer value from the text. You can also use <code>Scalar::string()</code>, <code>Scalar::boolean()</code> and <code>Scalar::float()</code> to extract other types of values.</p> <p>Additionally, you can use Scalar adapter to extract one of the provided options.</p> <pre><code>use Cognesy\\Instructor;\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he currently plays Doom Eternal.\",\n    responseModel: Scalar::select(\n        name: 'activityType',\n        options: ['work', 'entertainment', 'sport', 'other']\n    ),\n);\n\nvar_dump($value);\n// string(4) \"entertainment\"\n</code></pre> <p>NOTE: Currently Scalar::select() always returns strings and its <code>options</code> parameter only accepts string values.</p>"},{"location":"sequences/","title":"Sequences","text":""},{"location":"sequences/#extracting-sequences-of-objects","title":"Extracting Sequences of Objects","text":"<p>Sequence is a wrapper class that can be used to represent a list of objects to be extracted by Instructor from provided context.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <pre><code>class Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Sequence::of(Person::class),\n);\n</code></pre>"},{"location":"sequences/#streaming-sequences","title":"Streaming Sequences","text":"<p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p> <p>NOTE This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive sequence updates provide a callback via Instructor's <code>onSequenceUpdate()</code> that will be called each  time a new item is received from LLM.</p> <p>The callback provided a full sequence that has been retrieved so far. You can get the last added object from the sequence via <code>$sequence-&gt;last()</code>.</p> <p>Remember that while the sequence is being updated, the data is not validated - only when the sequence is fully extracted, the objects are validated and a full sequence is returned (see example below).</p> <pre><code>class Person\n{\n    public string $name;\n    public int $age;\n}\n\nfunction updateUI(Person $person) {\n    // add newly extracted person to the UI list\n    $this-&gt;ui-&gt;appendToList($person);\n    // remember those objects are not validated yet\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new Instructor)-&gt;request(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Sequence::of(Person::class),\n    options: ['stream' =&gt; true]\n)-&gt;onSequenceUpdate(\n    fn($sequence) =&gt; updateUI($sequence-&gt;last()) // get last added object\n)-&gt;get();\n\n// now the list is fully extracted and validated\nforeach ($list as $person) {\n    // do something with each person\n    $this-&gt;db-&gt;save($person);\n}\n</code></pre>"},{"location":"sequences/#working-with-sequences","title":"Working with Sequences","text":"<p>Sequences offer array access (via ArrayAccess) and convenience methods to work with the list of extracted objects.</p> <pre><code>$sequence-&gt;count();   // returns the number of extracted items\n$sequence-&gt;first();   // returns the first extracted item\n$sequence-&gt;last();    // returns the last extracted item\n$sequence-&gt;get(1);    // returns the second extracted item\n$sequence-&gt;toArray(); // returns the list of extracted items as an array\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#basic-usage","title":"Basic usage","text":"<p>This is a simple example demonstrating how Instructor retrieves structured information from provided text (or chat message sequence).</p> <p>Response model class is a plain PHP class with typehints specifying the types of fields of the object.</p> <p>NOTE: By default, Instructor looks for OPENAI_API_KEY environment variable to get your API key. You can also provide the API key explicitly when creating the Instructor instance.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor;\n\n// Step 0: Create .env file in your project root:\n// OPENAI_API_KEY=your_api_key\n\n// Step 1: Define target data structure(s)\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n// Step 2: Provide content to process\n$text = \"His name is Jason and he is 28 years old.\";\n\n// Step 3: Use Instructor to run LLM inference\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n);\n\n// Step 4: Work with structured response data\nassert($person instanceof Person); // true\nassert($person-&gt;name === 'Jason'); // true\nassert($person-&gt;age === 28); // true\n\necho $person-&gt;name; // Jason\necho $person-&gt;age; // 28\n\nvar_dump($person);\n// Person {\n//     name: \"Jason\",\n//     age: 28\n// }    \n</code></pre> <p>Note</p> <p>Currently, Instructor for PHP only supports classes / objects as response models. In case you want to extract simple types or arrays, you need to wrap them in a class.</p>"},{"location":"usage/#string-as-input","title":"String as Input","text":"<p>You can provide a string instead of an array of messages. This is useful when you want to extract data from a single block of text and want to keep your code simple.</p> <pre><code>use Cognesy\\Instructor;\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n);\n</code></pre>"},{"location":"usage/#alternative-ways-to-call-instructor","title":"Alternative ways to call Instructor","text":"<p>You can call <code>request()</code> method to set the parameters of the request and then call <code>get()</code> to get the response.</p> <pre><code>use Cognesy\\Instructor;\n$instructor = (new Instructor)-&gt;request(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n);\n$person = $instructor-&gt;get();\n</code></pre> <p>You can also initialize Instructor with a request object.</p> <pre><code>use Cognesy\\Instructor;\nuse Cognesy\\Instructor\\Data\\Request;\n\n$instructor = (new Instructor)-&gt;withRequest(new Request(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n))-&gt;get();\n</code></pre>"},{"location":"usage/#custom-openai-client","title":"Custom OpenAI client","text":"<p>You can provide your own OpenAI client to Instructor. This is useful when you want to initialize OpenAI client with custom values - e.g. to call other LLMs which support OpenAI API.</p> <pre><code>use Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n/ Create instance of OpenAI client initialized with custom parameters for Ollama\n$client = new OpenAIClient(\n    apiKey: 'ollama',\n    baseUri: 'http://localhost:11434/v1',\n    connectTimeout: 3,\n    requestTimeout: 60, // set based on your machine performance :)\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'llama2',\n    mode: Mode::MdJson\n    //options: ['stream' =&gt; true ]\n);\n\ndump($user);\n</code></pre>"},{"location":"validation/","title":"Validation","text":""},{"location":"validation/#basic-validation","title":"Basic validation","text":"<p>Instructor validates results of LLM response against validation rules specified in your data model.</p> <p>Note</p> <p>For further details on available validation rules, check Symfony Validation constraints.</p> <pre><code>&lt;?php\n\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is Jason, he is -28 years old.\";\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n);\n\n// if the resulting object does not validate, Instructor throws an exception\n</code></pre>"},{"location":"validation/#max-retries","title":"Max Retries","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/SelfCorrection/run.php</code></p> <p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p> <p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <pre><code>&lt;?php\n\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    #[Assert\\Length(min: 3)]\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is JX, aka Jason, he is -28 years old.\";\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    maxRetries: 3,\n);\n\n// if all LLM's attempts to self-correct the results fail, Instructor throws an exception\n</code></pre>"},{"location":"validation/#custom-validation","title":"Custom Validation","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/ValidationMixin/run.php</code></p> <p>You can easily add custom validation code to your response model by using <code>ValidationTrait</code> and defining validation logic in <code>validate()</code> method.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\Traits\\ValidationMixin;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $age;\n\n    public function validate() : array {\n        if ($this-&gt;name === strtoupper($this-&gt;name)) {\n            return [];\n        }\n        return [[\n            'message' =&gt; \"Name must be in uppercase.\",\n            'path' =&gt; 'name',\n            'value' =&gt; $this-&gt;name\n        ]];\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>Note that method <code>validate()</code> has to return:  * an empty array if the object is valid,  * or an array of validation violations.</p> <p>This information will be used by LLM to make subsequent attempts to correct the response.</p> <pre><code>$violations = [\n    [\n        'message' =&gt; \"Error message with violation details.\",\n        'path' =&gt; 'path.to.property',\n        'value' =&gt; '' // invalid value\n    ],\n    // ...other violations\n];\n</code></pre>"},{"location":"validation/#custom-validation-via-symfony-assertcallback","title":"Custom Validation via Symfony #[Assert/Callback]","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/CustomValidator/run.php</code></p> <p>Instructor uses Symfony validation component to validate extracted data.</p> <p>You can use <code>#[Assert/Callback]</code> annotation to build fully customized validation logic.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>Note</p> <p>See Symfony docs for more details on how to use Callback constraint.</p>"},{"location":"why/","title":"Why use Instructor?","text":"<p>Our library introduces three key enhancements:</p> <ul> <li>Response Mode: Specify a PHP model to streamline data extraction.</li> <li>Validation Context: Provide a context object for enhanced validator access.</li> <li>Max Retries: Set your desired number of retry attempts for requests.</li> </ul>"},{"location":"why/#a-glimpse-into-instructors-capabilities","title":"A Glimpse into Instructor's Capabilities","text":"<p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek.</p>"},{"location":"why/#understanding-the-workflow","title":"Understanding the workflow","text":"<p>Lets go over the <code>patch</code> function. And see how we can leverage it to make use of instructor</p>"},{"location":"why/#step-1-define-the-data-model","title":"Step 1: Define the data model","text":"<p>Create a data model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>class UserDetail {\n    public string $name;\n    public int $age;\n}\n</code></pre>"},{"location":"why/#step-2-extract","title":"Step 2: Extract","text":"<p>Use the <code>Instructor::respond()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies the model to use for extraction.</p> <p><pre><code>/** @var UserDetail */\n$user = (new Instructor)-&gt;respond(\n    messages: [[\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"]],\n    responseModel: UserDetail::class,\n    model: \"gpt-3.5-turbo\",\n);\n\nassert($user-&gt;name == \"Jason\")\nassert($user-&gt;age == 25)\n</code></pre> It's helpful to annotate the variable with the type of the response model, which will help your IDE provide autocomplete and spell check.</p>"},{"location":"why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same data model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Instructor will raise a validation error.</p> <pre><code>class QuestionAnswer:\n    public string $question;\n    public string answer: Annotated[\n        str, BeforeValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre> <p>Its important to note here that the error message is generated by the LLM, not the code, so it'll be helpful for re-asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>LeadReport</code> model is passed as the <code>$responseModel</code>, and <code>$maxRetries</code> is set to 2. It means that if the extracted data does not match the model, Instructor will re-ask the model 2 times before giving up.</p> <pre><code>use Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via jason@gmailcom -- Jason\"]],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\nassert($user-&gt;email === \"jason@gmail.com\");\n</code></pre> <p>More about Validation</p> <p>Check out Jason's blog post Good LLM validation is just good validation</p>"},{"location":"why/#custom-validators","title":"Custom Validators","text":"<p>Instructor uses Symfony validation component to validate extracted data. You can use #[Assert/Callback] annotation to build fully customized validation logic.</p> <p>See Symfony docs for more details on how to use Callback constraint.</p> <pre><code>    use Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\n    class UserDetails\n    {\n        public string $name;\n        public int $age;\n\n            #[Assert\\Callback]\n            public function validateName(ExecutionContextInterface $context, mixed $payload) {\n                if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n                    $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                        -&gt;atPath('name')\n                        -&gt;setInvalidValue($this-&gt;name)\n                        -&gt;addViolation();\n                }\n            }\n        }\n\n        $user = (new Instructor)-&gt;respond(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    );\n\n    assert($user-&gt;name === \"JASON\");\n</code></pre>"},{"location":"why/#sequences-iterables-lists","title":"Sequences (iterables / lists)","text":"<p>See: Sequences</p>"},{"location":"why/#partial-extraction","title":"Partial Extraction","text":"<p>See: Partial Extraction</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"concepts/classification/","title":"Text Classification using LLM","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using LLM (via OpenAI API), PHP's <code>enums</code> and classes.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using language models in combination with PHP data structures.</p>"},{"location":"concepts/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"concepts/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p> <pre><code>&lt;?php\n\n// Enumeration for single-label text classification. \nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction. \nclass SinglePrediction {\n    public Label $classLabel;\n}\n</code></pre>"},{"location":"concepts/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\Instructor;\n\n/**\n * Perform single-label classification on the input text. \n */\nfunction classify(string $data) : SinglePrediction {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n        model: \"gpt-3.5-turbo-0613\",\n    );\n}\n</code></pre>"},{"location":"concepts/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\nassert($prediction-&gt;classLabel == Label::SPAM);\n</code></pre>"},{"location":"concepts/classification/#multi-label-classification","title":"Multi-Label Classification","text":"<p>Example</p> <p>Run example from CLI: <code>php examples/ClassificationMulticlass/run.php</code></p>"},{"location":"concepts/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\n\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass Ticket {\n    /** @var Label[] */\n    public array $ticketLabels = [];\n}\n</code></pre>"},{"location":"concepts/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\Instructor;\n\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : Ticket {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify following support ticket: {$data}\",\n        ]],\n        responseModel: Ticket::class,\n        model: \"gpt-3.5-turbo-0613\",\n    );\n}\n</code></pre>"},{"location":"concepts/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;classLabels));\nassert(in_array(Label::BILLING, $prediction-&gt;classLabels));\n</code></pre>"},{"location":"concepts/philosophy/","title":"Philosophy","text":"<p>Note</p> <p>Philosophy behind Instructor was formulated by Jason Liu, the creator of original version of Instructor in Python and adapted for the PHP port.</p> <p>Instructor values simplicity and flexibility in leveraging language models. It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions.</p> <p>\u201cSimplicity is a great virtue, but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"concepts/philosophy/#simplicity","title":"Simplicity","text":"<ol> <li>Most users will only need to learn <code>responseModel</code> and <code>Instructor::respond()</code> to get started.</li> <li>No new prompting language to learn, no new abstractions to learn.</li> </ol>"},{"location":"concepts/philosophy/#transparency","title":"Transparency","text":"<ol> <li>We write very little prompts, and we don't try to hide the prompts from you.</li> <li>We give you config over the prompts we do write ('reasking' and in the future - JSON_MODE prompts).</li> </ol>"},{"location":"concepts/philosophy/#flexibility","title":"Flexibility","text":"<ol> <li>If you build a system with OpenAI directly, it is easy to incrementally adopt Instructor by just adding <code>Instructor::respond()</code> with data schemas fed in via <code>responseModel</code>.</li> <li>Use any class to define your data schema (no need to inherit from some base class).</li> </ol>"},{"location":"concepts/philosophy/#the-zen-of-instructor","title":"The zen of <code>instructor</code>","text":"<p>Maintain the flexibility and power of PHP classes, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define data schema <code>&lt;?php class StructuredData { ... }</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>&lt;?php function extract($input) : StructuredData</code></li> <li>Define typed computations against your data with <code>&lt;?php function compute(StructuredData $data):</code> or call methods on your schema <code>&lt;?php $data-&gt;compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"concepts/philosophy/#our-goals","title":"Our Goals","text":"<p>The goal for the library, documentation, and blog, is to help you be a better programmer and, as a result, a better AI engineer.</p> <ul> <li>The library is a result of our desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>We won't try to write prompts for you,</li> <li>We don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit us up @jnxlco or @ddebowczyk</p> <p>Cheers!</p>"},{"location":"concepts/prompting/","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor for function calling is to make the models self-descriptive, modular, and flexible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use PHPDoc comments or #[Description('')] attribute for clear field descriptions.</li> <li>Optionality: Use PHP's nullable types (e.g. ?int) for optional fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"concepts/prompting/#utilize-nullable-attribute","title":"Utilize Nullable Attribute","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/OptionalFields/run.php</code></p> <p>Use PHP's nullable types by prefixing type name with question mark (?) and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>&lt;?php\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?Role $role = null; \n}\n</code></pre>"},{"location":"concepts/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/HandlingErrors/run.php</code></p> <p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>&lt;?php\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?string $role = null;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $result = null;\n    public ?string $errorMessage = '';\n    public bool $error = false;\n\n    public function get(): ?UserDetail\n    {\n        return $this-&gt;error ? null : $this-&gt;result;\n    }\n}\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in 'errorMessage'.</p> <p>Original Instructor implementation in Python provides utility class Maybe making this pattern even easier. Such mechanism is not yet available in PHP version of Instructor.</p>"},{"location":"concepts/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>&lt;?php\n\nenum Role : string {\n    case Principal = 'principal'\n    case Teacher = 'teacher'\n    case Student = 'student'\n    case Other = 'other'\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /**  Correctly assign one of the predefined roles to the user. */\n    public Role $role;\n}\n</code></pre> <p>If you'd like to improve LLM inference performance, try reiterating the requirements in the field descriptions (in the docstrings).</p>"},{"location":"concepts/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/RestatingInstruction/run.php</code></p> <p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>&lt;?php\n\n/** Extract the role based on the following rules: &lt;your rules go here&gt; */\nclass Role\n{\n    /** Restate the instructions and rules to correctly determine the title. */\n    public string $instructions;\n    public string $title;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"concepts/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/ArbitraryProperties/run.php</code></p> <p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>&lt;?php\n\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n</code></pre>"},{"location":"concepts/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/LimitingLengthOfLists/run.php</code></p> <p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>&lt;?php\n\nclass Property\n{\n    /**  Monotonically increasing ID */\n    public string $index; \n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age\n    public string $name;\n    /** @var Property[] Numbered list of arbitrary extracted properties, should be less than 3 */\n    public array $properties;\n}\n</code></pre> <p>To be 100% certain the list does not exceed the limit add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"concepts/prompting/#consistent-arbitrary-properties","title":"Consistent Arbitrary Properties","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/ArbitraryPropertiesConsistency/run.php</code></p> <p>For multiple records containing arbitrary properties, instruct LLM to use consistent key names when extracting properties.</p> <pre><code>&lt;?php\n\nclass Property {\n    public int $id;\n    public string $key;\n    public string $name;\n}\n\nclass UserDetails\n{\n    /** @var UserDetail[] Extract information for multiple users. Use consistent key names for properties across users. */\n    public array $users;\n}\n</code></pre>"},{"location":"concepts/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/EntityRelationships/run.php</code></p> <p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> field:</p> <pre><code>&lt;?php\n\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /** @var int[] Correct and complete list of coworker IDs, representing collaboration between users. */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /** @var UserDetail[] Collection of users, correctly capturing the relationships among them. */\n    public array $users;\n}\n</code></pre>"},{"location":"concepts/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/ChainOfThought/run.php</code></p> <p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>&lt;?php\n\nclass Role\n{\n    /** Think step by step to determine the correct title. */\n    public string $chainOfThought = '';\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"concepts/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/TimeRange/run.php</code></p> <p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <pre><code>&lt;?php\n\n\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public int $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n</code></pre>"},{"location":"concepts/prompting/#adding-context-to-components","title":"Adding Context to Components","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/TimeRangeWithCoT/run.php</code></p> <p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>&lt;?php\n\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n</code></pre>"},{"location":"concepts/search/","title":"Expanding Search Queries","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"concepts/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>SearchQuery</code> class is a PHP class that defines the structure of an individual search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>&lt;?php\n\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n</code></pre>"},{"location":"concepts/search/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries. It uses the <code>Instructor::respond</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\Instructor;\n\nfunction segment(string $data) : Search {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Consider the data below: '\\n$data' and segment it into multiple search queries\",\n        ]],\n        responseModel: Search::class,\n    );\n}\n\nforeach (segment(\"Search for a picture of a cat and a video of a dog\")-&gt;queries as $query) {\n    $query-&gt;execute();\n    // dump($query);\n}\n</code></pre>"},{"location":"examples/","title":"Function Calls by Example","text":""},{"location":"examples/#quick-links","title":"Quick Links","text":"<ol> <li>How are single and multi-label classifications done using enums?</li> <li>How are search queries segmented through function calling and typed arrays?</li> </ol> <p>Explore more examples in the examples directory.</p>"},{"location":"hub/","title":"Instructor Hub","text":"<p>Welcome to Instructor Hub. The goal of this section is to provide a set of tutorials and examples to help you get started, and allow you to pull in the code you need to get started with Instructor.</p>"},{"location":"hub/#contributing","title":"Contributing","text":"<p>We welcome contributions to the instructor hub, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ol> <li>The code must be in a single .php file.</li> <li>Please include documentation in the file - check existing examples for the format.</li> <li>Make sure that the code is tested.</li> </ol>"},{"location":"hub/#cli-usage","title":"CLI Usage","text":"<p>Instructor hub comes with a command line interface (CLI) that allows you to view and interact with the tutorials and examples and allows you to pull in the code you need to get started with the API.</p>"},{"location":"hub/#list-cookbooks","title":"List Cookbooks","text":"<p>Run <code>./hub.sh list</code> you can see all the available tutorials and examples.</p> <p><pre><code>$ ./hub.sh list\n</code></pre> ...or on Windows:</p> <pre><code>$ ./hub.bat list\n</code></pre>"},{"location":"hub/#reading-a-cookbook","title":"Reading a Cookbook","text":"<p>To read a tutorial, you can run <code>./hub.sh show {id}</code> to see the full tutorial in the terminal.</p> <pre><code>$ ./hub.sh show {name or id}\n</code></pre> <p>Currently, there is no way to page through the tutorial - feel free to contribute :)</p>"},{"location":"hub/#running-a-cookbook","title":"Running a Cookbook","text":"<p>To run a tutorial, you run <code>./hub.sh run {id}</code> in terminal - it will execute the code and show the output. You need to have your OPENAI_API_KEY set in your environment (.env file in root directory of your copy of instructor-php repo). </p> <pre><code>$ ./hub.sh run {name or id}\n</code></pre>"},{"location":"hub/#running-all-cookbooks","title":"Running all Cookbooks","text":"<p>This is mostly for testing if cookbooks are executed properly, but you can run <code>./hub.sh all {id}</code> to run all the tutorials and examples in the terminal.</p> <pre><code>$ ./hub.sh all {name or id}\n</code></pre>"},{"location":"hub/#call-for-contributions","title":"Call for Contributions","text":"<p>We're looking for a bunch more hub examples, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ul> <li> Converting the cookbooks to the new format</li> <li> Validator examples</li> <li> Data extraction examples</li> <li> Streaming examples (Iterable and Partial)</li> <li> Batch Parsing examples</li> <li> Query Expansion examples</li> <li> Batch Data Processing examples</li> <li> Batch Data Processing examples with Cache</li> </ul> <p>We're also looking for help to catch up with the features available in Instructor Hub for Python (see: https://github.com/jxnl/instructor/blob/main/docs/hub/index.md).</p> <ul> <li> Better viewer with pagination</li> <li> Examples database</li> <li> Pulling in the code to your own dir, so you can get started with the API</li> </ul>"},{"location":"hub/arbitrary_properties/","title":"Arbitrary properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\n\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n?&gt;\n</code></pre> <p>Now we can use this data model to extract arbitrary properties from a text message in a form that is easier for future processing.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives\n    in a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    mode: Mode::MdJson,\n);\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(!empty($user-&gt;properties));\n?&gt;\n</code></pre>"},{"location":"hub/arbitrary_properties_consistency/","title":"Consistent values of arbitrary properties","text":"<p>For multiple records containing arbitrary properties, instruct LLM to get more consistent key names when extracting properties.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserDetail\n{\n    public int $id;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetails\n{\n    /**\n     * @var UserDetail[] Extract information for multiple users.\n     * Use consistent key names for properties across users.\n     */\n    public array $users;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a Python programmer. Amanda is UX designer.\n    John is 40yo and he's CEO.\n    TEXT;\n\n$list = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetails::class,\n);\n\ndump($list);\n\nassert(!empty($list-&gt;users));\n?&gt;\n</code></pre>"},{"location":"hub/chain_of_thought/","title":"Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality, by eliciting LLM reasoning to self-explain approach to generating the response.</p> <p>With Instructor you can achieve a 'modular' CoT, where multiple explanations can be generated by LLM for different parts of the response, driving a more granular control and improvement of the response.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass Employee {\n    /** Think step by step to determine the correct year of employment. */\n    public string $chainOfThought;\n    public int $yearOfEmployment;\n}\n\n$text = 'He was working here for 5 years. Now, in 2019, he is a manager.';\n\n$employee = (new Instructor)-&gt;respond(\n    [['role' =&gt; 'user', 'content' =&gt; $text]],\n    Employee::class\n);\n\ndump($employee);\n\nassert($employee-&gt;yearOfEmployment === 2014);\n?&gt;\n</code></pre>"},{"location":"hub/classification/","title":"Single label classification","text":""},{"location":"hub/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n// Enumeration for single-label text classification.\nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction.\nclass SinglePrediction {\n    public Label $classLabel;\n}\n?&gt;\n</code></pre>"},{"location":"hub/classification/#classifying-text","title":"Classifying Text","text":"<p>The function classify will perform the single-label classification.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction classify(string $data) : SinglePrediction {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n    );\n}\n?&gt;\n</code></pre>"},{"location":"hub/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\n\ndump($prediction);\n\nassert($prediction-&gt;classLabel == Label::SPAM);\n?&gt;\n</code></pre>"},{"location":"hub/classification_multiclass/","title":"Multiclass classification","text":""},{"location":"hub/classification_multiclass/#defining-the-structures","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass TicketLabels {\n    /** @var Label[] */\n    public array $labels = [];\n}\n?&gt;\n</code></pre>"},{"location":"hub/classification_multiclass/#classifying-text","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : TicketLabels {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Label following support ticket: {$data}\",\n        ]],\n        responseModel: TicketLabels::class,\n    );\n}\n?&gt;\n</code></pre>"},{"location":"hub/classification_multiclass/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\ndump($prediction);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;labels));\nassert(in_array(Label::BILLING, $prediction-&gt;labels));\n?&gt;\n</code></pre>"},{"location":"hub/custom_client_parameters/","title":"Customize parameters of OpenAI client","text":"<p>You can provide your own OpenAI client instance to Instructor. This is useful when you want to initialize OpenAI client with custom values - e.g. to call other LLMs which support OpenAI API.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenAIClient(\n    apiKey: Env::get('OPENAI_API_KEY'),\n    baseUri: 'https://api.openai.com/v1',\n    connectTimeout: 3,\n    requestTimeout: 30,\n    metadata: ['organization' =&gt; ''],\n);\n\n// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)\n    -&gt;withClient($client);\n\n// Call with custom model and execution mode\n$user = $instructor-&gt;respond(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    model: 'gpt-3.5-turbo',\n    mode: Mode::Json,\n);\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/custom_validator/","title":"Custom validation using Symfony Validator","text":"<p>Instructor uses Symfony validation component to validate properties of extracted data. Symfony offers you #[Assert/Callback] annotation to build fully customized validation logic.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\n\nuse Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\ndump($user);\n\nassert($user-&gt;name === \"JASON\");\n?&gt;\n</code></pre>"},{"location":"hub/entity_relationships/","title":"Entity Relationship Extraction","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> fields.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /**\n     * @var int[] Correct and complete list of coworker IDs, representing\n     * collaboration between users.\n     */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /**\n     * @var UserDetail[] Collection of users, correctly capturing the\n     * relationships among them.\n     */\n    public array $users;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a Python programmer of Apex website.\n    Amanda is a contractor working with Jason on Apex website. John is\n    40yo and he's CEO - Jason reports to him.\n    TEXT;\n\n$relationships = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserRelationships::class,\n);\n\ndump($relationships);\n\nassert(!empty($relationships-&gt;users));\n?&gt;\n</code></pre>"},{"location":"hub/handling_errors/","title":"Handling errors","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $user = null;\n    public bool $noUserData = false;\n    /** If no user data, provide reason */\n    public ?string $errorMessage = '';\n\n    public function get(): ?UserDetail\n    {\n        return $this-&gt;noUserData ? null : $this-&gt;user;\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    [['role' =&gt; 'user', 'content' =&gt; 'We don\\'t know anything about this guy.']],\n    MaybeUser::class\n);\n\ndump($user);\n\nassert($user-&gt;noUserData);\nassert(!empty($user-&gt;errorMessage));\nassert($user-&gt;get() === null);\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_anthropic/","title":"Support for Anthropic API","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - Mode::MdJson, Mode::Json - supported - Mode::Tools - not supported yet</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Anthropic\\AnthropicClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Create instance of client initialized with custom parameters\n$client = new AnthropicClient(\n    apiKey: Env::get('ANTHROPIC_API_KEY'),\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'claude-3-haiku-20240307',\n    mode: Mode::Tools,\n    //options: ['stream' =&gt; true ]\n);\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_anyscale/","title":"Support for Anyscale API","text":"<p>Anyscale is hosted language model provider that offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Anyscale as demonstrated below.</p> <p>Please note that some models support Mode::Tools or Mode::Json, which are much more reliable than Mode::MdJson.</p> <p>Mode compatibility: - Mode::Tools - selected models - Mode::Json - selected models - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Anyscale\\AnyscaleClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('ANYSCALE_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new AnyscaleClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n        mode: Mode::Json,\n        //options: ['stream' =&gt; true ]\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_azure_o_a_i/","title":"Azure support","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Azure\\AzureClient;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n/// Custom client parameters: base URI\n$resourceName = Env::get('AZURE_OPENAI_RESOURCE_NAME'); // set your own value/source\n\n$client = (new AzureClient(\n    apiKey: Env::get('AZURE_OPENAI_API_KEY'),\n    resourceName: 'instructor-dev', // set your own value/source\n    deploymentId: 'gpt-35-turbo-16k', // set your own value/source\n    apiVersion: '2024-02-01',\n));\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n// Call with your model name and preferred execution mode\n$user = $instructor-&gt;respond(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    model: 'gpt-35-turbo-16k', // set your own value/source\n    //options: ['stream' =&gt; true ]\n);\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_fireworks_a_i/","title":"Support for Fireworks.ai API","text":"<p>Please note that the larger Mistral models support Mode::Json, which is much more reliable than Mode::MdJson.</p> <p>Mode compatibility: - Mode::Tools - selected models - Mode::Json - selected models - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\FireworksAI\\FireworksAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('FIREWORKSAI_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new FireworksAIClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'accounts/fireworks/models/mixtral-8x7b-instruct',\n        mode: Mode::Json,\n    //options: ['stream' =&gt; true ]\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_mistral/","title":"Support for Mistral API","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support Mode::Json, which is much more reliable than Mode::MdJson.</p> <p>Mode compatibility:  - Mode::Tools - Mistral-Small / Mistral-Medium / Mistral-Large  - Mode::Json - Mistral-Small / Mistral-Medium / Mistral-Large  - Mode::MdJson - Mistral 7B / Mixtral 8x7B</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Mistral\\MistralClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('MISTRAL_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new MistralClient(\n    apiKey: $yourApiKey,\n    baseUri: 'https://api.mistral.ai/v1',\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'mistral-small-latest',\n        mode: Mode::Json,\n        options: ['stream' =&gt; true ]\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_ollama/","title":"Support for local Ollama","text":"<p>You can use Instructor with local Ollama instance. Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Create instance of OpenAI client initialized with custom parameters for Ollama\n$client = new OpenAIClient(\n    apiKey: 'ollama',\n    baseUri: 'http://localhost:11434/v1',\n    connectTimeout: 3,\n    requestTimeout: 60, // set based on your machine performance :)\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'llama2',\n    mode: Mode::MdJson\n    //options: ['stream' =&gt; true ]\n);\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_open_a_i/","title":"Support for OpenAI API","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - Mode::Tools (recommended)  - Mode::Json  - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// OpenAI auth params\n$yourApiKey = Env::get('OPENAI_API_KEY'); // use your own API key\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenAIClient(\n    apiKey: $yourApiKey,\n    baseUri: 'https://api.openai.com/v1',\n    organization: '',\n    connectTimeout: 3,\n    requestTimeout: 30,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'gpt-3.5-turbo',\n    //options: ['stream' =&gt; true ]\n);\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_open_router/","title":"Support for OpenRouter API","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenRouter\\OpenRouterClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// OpenRouter client params\n$yourApiKey = Env::get('OPENROUTER_API_KEY'); // or your own value/source\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenRouterClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'mistralai/mixtral-8x7b-instruct:nitro',\n    mode: Mode::Json,\n    //options: ['stream' =&gt; true ]\n);\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/l_l_m_support_together_a_i/","title":"Support for Together.ai API","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support Mode::Tools or Mode::Json, which are much more reliable than Mode::MdJson.</p> <p>Mode compatibility: - Mode::Tools - selected models - Mode::Json - selected models - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\TogetherAI\\TogetherAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'Guest';\n    case User = 'User';\n    case Admin = 'Admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('TOGETHERAI_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new TogetherAIClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n        mode: Mode::Json,\n        //options: ['stream' =&gt; true ]\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/limiting_length_of_lists/","title":"Limiting the length of lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length of list. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <p>To be 100% certain the list does not exceed the limit, add extra validation, e.g. using ValidationMixin (see: Validation).</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Data\\ValidationResult;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Traits\\ValidationMixin;\n\nclass Property\n{\n    /**  Monotonically increasing ID, not larger than 2 */\n    public string $index;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    use ValidationMixin;\n\n    public int $age;\n    public string $name;\n    /** @var Property[] List other extracted properties - not more than 2. */\n    public array $properties;\n\n    public function validate() : ValidationResult\n    {\n        if (count($this-&gt;properties) &lt; 3) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'properties',\n            value: $this-&gt;name,\n            message: \"Number of properties must be not more than 2.\",\n        );\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives in\n    a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    maxRetries: 1 // change to 0 to see validation error\n);\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(count($user-&gt;properties) &lt; 3);\n?&gt;\n</code></pre>"},{"location":"hub/optional_fields/","title":"Making some fields optional","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) to mark component fields which are optional and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserRole\n{\n    public string $title;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?UserRole $role;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; \"Jason is 25 years old.\"]],\n    responseModel: UserDetail::class,\n);\n\ndump($user);\n\nassert(!isset($user-&gt;role));\n?&gt;\n</code></pre>"},{"location":"hub/partial_updates/","title":"Streaming partial updates during inference","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated. You can use it to improve user experience by updating the UI with partial data before the full response is received.</p> <p><pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public string $location;\n    /** @var UserRole[] */\n    public array $roles;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    echo chr(27).chr(91).'H'.chr(27).chr(91).'J';\n\n    // Print explanation\n    echo \"Waiting 250ms on every update received to make changes easier to observe...\\n\";\n\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$user = (new Instructor)-&gt;request(\n    messages: $text,\n    responseModel: UserDetail::class,\n    options: ['stream' =&gt; true],\n)-&gt;onPartialUpdate(partialUpdate(...))-&gt;get();\n\necho \"All tokens received, fully completed object available in `\\$user` variable.\\n\";\necho '$user = '.\"\\n\";\ndump($user);\n\nassert(!empty($user-&gt;roles));\nassert(!empty($user-&gt;hobbies));\nassert($user-&gt;location == 'San Francisco');\nassert($user-&gt;age == 25);\nassert($user-&gt;name == 'Jason');\n?&gt;\n</code></pre>"},{"location":"hub/private_vs_public_fields/","title":"Private vs public object field","text":"<p>Instructor only sets public fields of the object with the data provided by LLM. Private and protected fields are left unchanged. If you want to access them directly after extraction, consider providing default values for them.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass User\n{\n    public string $name;\n    public int $age;\n    public string $password = '';\n\n    public function getAge(): int {\n        return $this-&gt;age;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\nclass UserWithPrivateField\n{\n    public string $name;\n    protected int $age = 0;\n    private string $password = '';\n\n    public function getAge(): int {\n        return $this-&gt;age;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. His password is '123admin'.\n    TEXT;\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: User::class\n);\n\n$userPriv = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserWithPrivateField::class\n);\n\necho \"User with public 'password' field\\n\";\ndump($user);\n\necho \"User with private 'password' field\\n\";\ndump($userPriv);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;getAge() === 25);\nassert($user-&gt;getPassword() === '123admin');\n\nassert($userPriv-&gt;name === \"Jason\");\nassert($userPriv-&gt;getAge() === 0);\nassert($userPriv-&gt;getPassword() === '');\n?&gt;\n</code></pre>"},{"location":"hub/restating_instructions/","title":"Restating instructions","text":"<p>Make Instructor restate long or complex instructions and rules to improve inference accuracy.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /** Restate instructions and rules, so you can correctly determine the title. */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /* Guess job title */\n    public string $title;\n}\n\n/**\n * Details of analyzed user. The key information we're looking for\n * is appropriate role data.\n */\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am the head of Apex Software, responsible for\n    driving growth of our company.\n    TEXT;\n\n$instructor = new Instructor;\n$user = ($instructor)-&gt;respond(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n);\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\nassert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"hub/rewriting_instructions/","title":"Ask LLM to rewrite instructions","text":"<p>Asking LLM to rewrite the instructions and rules is another way to improve inference results.</p> <p>You can provide arbitrary instructions on the data handling in the class and property PHPDocs. Instructor will use these instructions to guide LLM in the inference process.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /**\n     * Rewrite the instructions and rules in a concise form to correctly\n     * determine the user's title - just the essence.\n     */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /** Most likely job title */\n    public string $title;\n}\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am responsible for driving growth of our\n    company.\n    TEXT;\n\n$instructor = new Instructor;\n$user = $instructor-&gt;respond(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n);\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\nassert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"hub/search_criteria/","title":"Expanding Search Queries","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"hub/search_criteria/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>SearchQuery</code> class is a PHP class that defines the structure of an individual search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n?&gt;\n</code></pre>"},{"location":"hub/search_criteria/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries. It uses the <code>Instructor::respond</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nfunction segment(string $data) : Search {\n    return (new Instructor)-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Consider the data below: '\\n$data' and segment it into multiple search queries\",\n        ]],\n        responseModel: Search::class,\n    );\n}\n\n$search = segment(\"Find a picture of a cat and a video of a dog\");\nforeach ($search-&gt;queries as $query) {\n    $query-&gt;execute();\n}\n// Results:\n// Searching with query `picture of a cat` using `image`\n// Searching with query `video of a dog` using `video`\n\nassert(count($search-&gt;queries) === 2);\n?&gt;\n</code></pre>"},{"location":"hub/self_correction/","title":"Automatic correction based on validation results","text":"<p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via jason@gmailcom -- Jason\"]],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\ndump($user);\n\nassert($user-&gt;email === \"jason@gmail.com\");\n?&gt;\n</code></pre>"},{"location":"hub/sequences/","title":"Extracting sequences of objects","text":"<p>Sequences are a special type of response model that can be used to represent a list of objects.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Extras\\Sequences\\Sequence;\nuse Cognesy\\Instructor\\Instructor;\n\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old. Anna is 2 years younger than him.\n    TEXT;\n\n$list = (new Instructor)\n    -&gt;request(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        responseModel: Sequence::of(Person::class),\n        options: ['stream' =&gt; true]\n    )\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; dump($sequence-&gt;last()))\n    -&gt;get();\n\ndump(count($list));\n\nassert(count($list) === 4);\n?&gt;\n</code></pre>"},{"location":"hub/time_range/","title":"Reusing components","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public string $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Yesterday Jason worked from 9 for 5 hours. Later I watched 2 hour movie which I finished at 19.\"]],\n    responseModel: UserDetail::class,\n    maxRetries: 2\n);\n\ndump($user);\n\nassert($user-&gt;name == \"Jason\");\nassert($user-&gt;workTime-&gt;startTime === 9);\nassert($user-&gt;workTime-&gt;endTime === 14);\nassert($user-&gt;leisureTime-&gt;startTime === 17);\nassert($user-&gt;leisureTime-&gt;endTime === 19);\n?&gt;\n</code></pre>"},{"location":"hub/time_range_with_co_t/","title":"Using CoT to improve interpretation of component data","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <p>We're additionally starting the data structure with a Chain of Thought field to elicit LLM reasoning for the time range calculation, which can improve the accuracy of the response.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours (0-23 format) */\n    public int $startTime;\n    /** The end time in hours (0-23 format) */\n    public int $endTime;\n}\n\n$timeRange = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Workshop with Apex Industries started 9 and it took us 6 hours to complete.\"]],\n    responseModel: TimeRange::class,\n    maxRetries: 2\n);\n\ndump($timeRange);\n\nassert($timeRange-&gt;startTime === 9);\nassert($timeRange-&gt;endTime === 15);\n?&gt;\n</code></pre>"},{"location":"hub/validation_mixin/","title":"ValidationMixin","text":"<p>Sometimes property level validation is not enough - you may want to check values of multiple properties and based on the combination of them decide to accept or reject the response. Or the assertions provided by Symfony may not be enough for your use case.</p> <p>In such case you can easily add custom validation code to your response model by: - using <code>ValidationTrait</code> - and defining validation logic in <code>validate()</code> method.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Data\\ValidationResult;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Traits\\ValidationMixin;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $age;\n\n    public function validate() : ValidationResult {\n        if ($this-&gt;name === strtoupper($this-&gt;name)) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'name',\n            value: $this-&gt;name,\n            message: \"Name must be in uppercase.\",\n        );\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\ndump($user);\n\nassert($user-&gt;name === \"JASON\");\n?&gt;\n</code></pre>"},{"location":"hub/wiretap/","title":"Receive and process Instructor's internal events","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <ul> <li><code>(new Instructor)-&gt;onEvent(string $class, callable $callback)</code> method - receive callback when specified type of event is dispatched</li> <li><code>(new Instructor)-&gt;wiretap(callable $callback)</code> method - receive any event dispatched by Instructor, may be useful for debugging or performance analysis</li> <li><code>(new Instructor)-&gt;onError(callable $callback)</code> method - receive callback on any uncaught error, so you can customize handling it, for example logging the error or using some fallback mechanism in an attempt to recover</li> </ul> <p>Receiving events can help you to monitor the execution process and makes it easier for a developer to understand and resolve any processing issues.</p> <p>This is an example of wiretapping to receive all events dispatched by Instructor during the processing of a request.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nenum Role : string {\n    case CEO = 'ceo';\n    case CTO = 'cto';\n    case Developer = 'developer';\n    case Other = 'other';\n}\n\nclass UserDetail\n{\n    public string $name;\n    public Role $role;\n    public int $age;\n}\n\n$user = (new Instructor)\n    -&gt;request(\n        messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; \"Contact our CTO, Jason is 28 years old -- Tom\"]],\n        responseModel: UserDetail::class,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name == \"Jason\");\nassert($user-&gt;role == Role::CTO);\nassert($user-&gt;age == 28);\n?&gt;\n</code></pre>"},{"location":"internals/","title":"Internals","text":"<p>You can find some information on Instructor internals here, which should allow you better understand how it is designed and how it works.</p> <ul> <li>Instructor</li> <li>Configuration</li> <li>Lifecycle</li> <li>Response Models</li> <li>Contracts</li> <li>Events</li> </ul>"},{"location":"internals/configuration/","title":"Configuration","text":"<p>Instructor uses <code>Configuration</code> and <code>ConmponentConfig</code> classes to handle configuration of all components of the library.</p>"},{"location":"internals/configuration/#configuration-class","title":"<code>Configuration</code> class","text":"<p>Instructor uses <code>Configuration</code> class to handle configuration of all components of the library. It is used to define components, the way to instantiate them and the dependencies between components.</p> <p><code>Configuration</code> class is responsible for instantiation of the components (and inject them with the configuration data).</p>"},{"location":"internals/configuration/#componentconfig-class","title":"<code>ComponentConfig</code> class","text":"<p><code>ComponentConfig</code> class contains a configuration data of a single component.</p>"},{"location":"internals/configuration/#global-autowire-function","title":"Global <code>autowire()</code> function","text":"<p>Instructor comes with a global <code>autowire()</code> function containing a default wiring between components used by the library. It is located in <code>config\\autowire.php</code> file.</p>"},{"location":"internals/contracts/","title":"Response model contracts","text":"<p>Instructor allows you to customize processing of $responseModel value also by looking at the interfaces the class or instance implements:</p> <ul> <li><code>CanProvideJsonSchema</code> - implement to be able to provide raw JSON Schema (as an array) of the response model, overriding the default approach of Instructor, which is analyzing $responseModel value class information,</li> <li><code>CanProvideSchema</code> - implement to be able to provide <code>Schema</code> object of the response model, overriding class analysis stage; can be useful in building object wrappers (see: <code>Sequence</code> class),</li> <li><code>CanDeserializeSelf</code> - implement to customize the way the response from LLM is deserialized from JSON into PHP object,</li> <li><code>CanValidateSelf</code> - implement to customize the way the deserialized object is validated - it fully replaces the default validation process for given response model,</li> <li><code>CanTransformSelf</code> - implement to transform the validated object into any target value that will be then passed back to the caller (e.g. unwrap simple type from a class to scalar value)</li> </ul> <p>Methods implemented by those interfaces are executed is following:  -   - CanProvideJsonSchema - executed during the schema generation phase,  - CanDeserializeSelf - executed during the deserialization phase,  - CanValidateSelf - executed during the validation phase,  - CanTransformSelf - executed during the transformation phase.</p> <p>When implementing custom response handling strategy, avoid doing all transformations in a single block of code. Split the logic between relevant methods implemented by your class for clarity and easier code maintenance.</p>"},{"location":"internals/contracts/#example-implementation","title":"Example implementation","text":"<p>For a practical example of using those contracts to customize Instructor processing flow see:</p> <ul> <li>src/Extras/Scalars/</li> <li>src/Extras/Sequence/</li> </ul> <p>Examples contain an implementation of custom $responseModel wrappers, e.g. providing scalar value response support with a wrapper class implementing custom schema provider, deserialization, validation and transformation into requested value type.</p>"},{"location":"internals/events/","title":"Events","text":""},{"location":"internals/events/#event-classes","title":"Event classes","text":"<p>Instructor dispatches multiple classes of events (all inheriting from <code>Event</code> class) during its execution. You can listen to these events and react to them in your application, for example to log information or to monitor the execution process.</p>"},{"location":"internals/events/#receiving-notification-on-internal-events","title":"Receiving notification on internal events","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <ul> <li><code>(new Instructor)-&gt;onEvent(string $class, callable $callback)</code> method - receive callback when specified type of event is dispatched</li> <li><code>(new Instructor)-&gt;wiretap(callable $callback)</code> method - receive any event dispatched by Instructor, may be useful for debugging or performance analysis</li> <li><code>(new Instructor)-&gt;onError(callable $callback)</code> method - receive callback on any uncaught error, so you can customize handling it, for example logging the error or using some fallback mechanism in an attempt to recover</li> </ul> <p>Receiving events can help you to monitor the execution process and makes it easier for a developer to understand and resolve any processing issues.</p> <pre><code>$instructor = (new Instructor)\n    // see requests to LLM\n    -&gt;onEvent(RequestSentToLLM::class, fn($e) =&gt; dump($e))\n    // see responses from LLM\n    -&gt;onEvent(ResponseReceivedFromLLM::class, fn($event) =&gt; dump($event))\n    // see all events in console-friendly format\n    -&gt;wiretap(fn($event) =&gt; dump($event-&gt;toConsole()))\n    // log errors via your custom logger\n    -&gt;onError(fn($request, $error) =&gt; $logger-&gt;log($error));\n\n$instructor-&gt;respond(\n    messages: \"What is the population of Paris?\",\n    responseModel: Scalar::integer(),\n);\n// check your console for the details on the Instructor execution\n</code></pre>"},{"location":"internals/events/#convenience-methods-for-get-streamed-model-updates","title":"Convenience methods for get streamed model updates","text":"<p><code>Instructor</code> class provides convenience methods allowing client code to receive model updates  when streaming is enabled:</p> <ul> <li><code>onPartialUpdate(callable $callback)</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate(callable $callback)</code> - to handle partial sequence updates of the response</li> </ul> <p>In both cases your callback will receive updated model, so you don't have to extract it from the event.</p>"},{"location":"internals/instructor/","title":"<code>Instructor</code> class","text":"<p><code>Instructor</code> class is the main entry point to the library. It is responsible for handling all interactions with the client code and internal Instructor components.</p> <p>One of the essential tasks of the <code>Instructor</code> class is to read the configuration and use it to retrieve a component implementing <code>CanHandleRequest</code> interface (specified in the configuration) to process the request and return the response.</p> <p><code>Instructor</code> class dispatches several high level events during initialization and processing of the request and response:</p> <ul> <li><code>InstructorStarted</code> - dispatched when Instructor is created</li> <li><code>InstructorReady</code> - dispatched when Instructor is configured and ready to process the request</li> <li><code>RequestReceived</code> - dispatched when the request is received</li> <li><code>ResponseGenerated</code> - dispatched when the response is generated</li> <li><code>ErrorRaised</code> - dispatched when an uncaught error occurs</li> </ul> <p><code>Instructor</code> class contains top level try-catch block to let user handle any uncaught errors before throwing them to the client code.</p> <p><code>Instructor</code> class provides several methods allowing client code to plug into Instructor event system, including:  - <code>onEvent()</code> - to receive a callback when specified type of event is dispatched  - <code>wiretap()</code> - to receive any event dispatched by Instructor  - <code>onError()</code> - to receive callback on any uncaught error</p> <p>Additionally, <code>Instructor</code> class provides convenience methods allowing client code to receive model updates when streaming is enabled:</p> <ul> <li><code>onPartialUpdate()</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate()</code> - to handle partial sequence updates of the response</li> </ul>"},{"location":"internals/lifecycle/","title":"Lifecycle","text":"<p>As Instructor for PHP processes your request, it goes through several stages:</p> <ol> <li>Initialize and self-configure (with possible overrides defined by developer).</li> <li>Analyze classes and properties of the response data model specified by developer.</li> <li>Encode data model into a schema that can be provided to LLM.</li> <li>Execute request to LLM using specified messages (content) and response model metadata.</li> <li>Receive a response from LLM or multiple partial responses (if streaming enabled).</li> <li>Deserialize response received from LLM into originally requested classes and their properties.</li> <li>In case response contained incomplete or corrupted data - if errors are encountered, create feedback message for LLM and requests regeneration of the response.</li> <li>Execute validations defined by developer for the data model - if any of them fail, create feedback message for LLM and requests regeneration of the response.</li> <li>Repeat the steps 4-8, unless specified limit of retries has been reached or response passes validation</li> </ol>"},{"location":"internals/response_models/","title":"Response Models","text":"<p>Instructor is able to process several types of input provided as response model, giving you more flexibility on how you interact with the library.</p> <p>The signature of <code>respond()</code> method of Instructor states the <code>responseModel</code> can be either string, object or array.</p>"},{"location":"internals/response_models/#handling-string-responsemodel-value","title":"Handling string $responseModel value","text":"<p>If <code>string</code> value is provided, it is used as a name of the class of the response model.</p> <p>Instructor checks if the class exists and analyzes the class &amp; properties type information &amp; doc comments to generate a schema needed to specify LLM response constraints.</p> <p>The best way to provide the name of the response model class is to use <code>NameOfTheClass::class</code>, making it easy for IDE to check the type, handle refactorings, etc.</p>"},{"location":"internals/response_models/#handling-object-responsemodel-value","title":"Handling object $responseModel value","text":"<p>If <code>object</code> value is provided, it is considered an instance of the response model. Instructor checks the class of the instance, then analyzes it and its property type data to specify LLM response constraints.</p>"},{"location":"internals/response_models/#handling-array-responsemodel-value","title":"Handling array $responseModel value","text":"<p>If <code>array</code> value is provided, it is considered a raw JSON Schema, therefore allowing Instructor to use it directly in LLM requests (after wrapping in appropriate context - e.g. function call).</p> <p>Instructor requires information on the class of each nested object in your JSON Schema, so it can correctly deserialize the data into appropriate type.</p> <p>This information is available to Instructor when you are passing $responseModel as a class name or an instance, but it is missing from raw JSON Schema. Lack of the information on target class makes it impossible for Instructor to deserialize the data into appropriate, expected type.</p> <p>Current design uses JSON Schema <code>$comment</code> field on property to overcome this information gap. Instructor expects developer to use <code>$comment</code> field to provide fully qualified name of the target class to be used to deserialize property data of object or enum type.</p>"}]}